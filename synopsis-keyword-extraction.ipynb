{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install and import modules","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nimport ast\nfrom matplotlib import pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install sentence-transformers\n!pip install keybert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2022-02-28T14:25:17.130299Z","iopub.execute_input":"2022-02-28T14:25:17.131308Z","iopub.status.idle":"2022-02-28T14:25:21.360709Z","shell.execute_reply.started":"2022-02-28T14:25:17.131265Z","shell.execute_reply":"2022-02-28T14:25:21.359751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keybert import KeyBERT\nkw_model = KeyBERT()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T14:16:13.311099Z","iopub.execute_input":"2022-02-28T14:16:13.311512Z","iopub.status.idle":"2022-02-28T14:16:30.273263Z","shell.execute_reply.started":"2022-02-28T14:16:13.311466Z","shell.execute_reply":"2022-02-28T14:16:30.272197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and preprosess data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/myanimelist-dataset-animes-profiles-reviews/animes.csv')\ndf.dropna(inplace=True, subset=['synopsis', 'title'])\ndf = df[~df.uid.duplicated(keep='first')]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T14:18:26.191582Z","iopub.execute_input":"2022-02-28T14:18:26.192661Z","iopub.status.idle":"2022-02-28T14:18:26.486835Z","shell.execute_reply.started":"2022-02-28T14:18:26.192611Z","shell.execute_reply":"2022-02-28T14:18:26.485643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TITLE = 'Shingeki no Kyojin'\ndef get_synopsis(title):\n    doc = df[df['title'] == title].synopsis.values[0]\n    doc = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", doc) # Remove the \"written by\" caption\n    doc = doc.replace(u'\\n', u'').replace(u'\\r', u'')\n    doc = nlp(doc)\n    return doc\ndoc = get_synopsis(TITLE)\nprint(doc)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T14:45:23.927754Z","iopub.execute_input":"2022-02-28T14:45:23.928065Z","iopub.status.idle":"2022-02-28T14:45:23.983236Z","shell.execute_reply.started":"2022-02-28T14:45:23.928034Z","shell.execute_reply":"2022-02-28T14:45:23.98217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract keyword candidates","metadata":{}},{"cell_type":"code","source":"for np in doc.noun_chunks: # use np instead of np.text\n    if len(np) > 1:\n        print('> ', np)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T14:45:42.945832Z","iopub.execute_input":"2022-02-28T14:45:42.946481Z","iopub.status.idle":"2022-02-28T14:45:42.963986Z","shell.execute_reply.started":"2022-02-28T14:45:42.946386Z","shell.execute_reply":"2022-02-28T14:45:42.963211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_keyword_candidates(doc):\n    # code to recursively combine nouns\n    # 'We' is actually a pronoun but included in your question\n    # hence the token.pos_ == \"PRON\" part in the last if statement\n    # suggest you extract PRON separately like the noun-chunks above\n\n    index = 0\n    nounIndices = []\n    for token in doc:\n        if token.pos_ == 'NOUN':\n            nounIndices.append(index)\n        index = index + 1\n\n    print('Nouns found: ', len(nounIndices))\n\n    candidates = []\n    for idxValue in nounIndices:\n        if not bool(doc[idxValue].left_edge.ent_type_):\n            start = doc[idxValue].left_edge.i\n        else:\n            start = idxValue \n\n        if not bool(doc[idxValue].right_edge.ent_type_):\n            finish = doc[idxValue].right_edge.i+1\n        else:\n            finish = idxValue + 1\n\n        if finish-start > 0 and finish-start <7:\n            span = doc[start : finish]\n#             print('>', span)\n            candidates.append(span.text)\n\n    return candidates\n\nget_keyword_candidates(doc)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T14:53:52.587005Z","iopub.execute_input":"2022-02-28T14:53:52.588507Z","iopub.status.idle":"2022-02-28T14:53:52.604644Z","shell.execute_reply.started":"2022-02-28T14:53:52.588453Z","shell.execute_reply":"2022-02-28T14:53:52.603765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keywords = kw_model.extract_keywords(doc.text, candidates=candidates, \n                              use_mmr=True, diversity=0.7)\n\nkeywords","metadata":{"execution":{"iopub.status.busy":"2022-02-28T14:29:20.230195Z","iopub.execute_input":"2022-02-28T14:29:20.230536Z","iopub.status.idle":"2022-02-28T14:29:21.131716Z","shell.execute_reply.started":"2022-02-28T14:29:20.230493Z","shell.execute_reply":"2022-02-28T14:29:21.130713Z"},"trusted":true},"execution_count":null,"outputs":[]}]}